{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RWk_nZtxncVY",
    "outputId": "2adbbec9-2193-412b-d211-a65dcdd05dd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hlsYnL-mv-Ju"
   },
   "outputs": [],
   "source": [
    "ROOT='/content/drive/My Drive/dl/推荐评论/'\n",
    "BERT_ROOT='/content/drive/My Drive/dl/google-quest-challenge/pretrained-bert-models-for-pytorch/新建文件夹/bert-base-chinese/'\n",
    "BERT_TOK_ROOT='/content/drive/My Drive/dl/google-quest-challenge/pretrained-bert-models-for-pytorch/bert-base-chinese-vocab.txt'\n",
    "MAX_LEN=22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 776
    },
    "colab_type": "code",
    "id": "7kUv91JXxFCL",
    "outputId": "7c7c0653-dd70-4c12-c935-86bde3065812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.11.15)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.17.5)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
      "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n",
      "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.14.15)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch_pretrained_bert) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch_pretrained_bert) (2.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.15.0,>=1.14.15->boto3->pytorch_pretrained_bert) (1.12.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: scikit-learn==0.21.3 in /usr/local/lib/python3.6/dist-packages (0.21.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.3) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.3) (1.17.5)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.3) (1.4.1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.21.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install pytorch_pretrained_bert\n",
    "!pip install transformers\n",
    "!pip install scikit-learn==0.21.3\n",
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 62
    },
    "colab_type": "code",
    "id": "hB8UvNXqwQP4",
    "outputId": "db281f7b-4c42-4c22-e5d2-8e4a907d94d2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from pytorch_pretrained_bert import BertModel, BertTokenizer\n",
    "root='/content/drive/My Drive/google-quest-challenge/'\n",
    "import numpy as np\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import math\n",
    "import scipy\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import transformers, sys, os, gc\n",
    "from transformers import (\n",
    "    BertTokenizer, BertModel, BertForSequenceClassification, BertConfig,\n",
    "    WEIGHTS_NAME, CONFIG_NAME, AdamW, get_linear_schedule_with_warmup, \n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from torch.utils.data import DataLoader, Dataset,RandomSampler, SequentialSampler\n",
    "from transformers.modeling_bert import BertPreTrainedModel \n",
    "from sklearn.model_selection import StratifiedShuffleSplit,StratifiedKFold\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.metrics import roc_curve \n",
    "import numpy as np\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "IcJYytVUqGfF"
   },
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "setup_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "7hDBk0A3zQdi"
   },
   "outputs": [],
   "source": [
    "labels,train_data=[],[]\n",
    "test=[]\n",
    "weight=[]\n",
    "lenth = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "GrExBJ4AzUM2"
   },
   "outputs": [],
   "source": [
    "with open (ROOT+'data/train.txt', 'r') as f:\n",
    "  for line in f:\n",
    "    label,text = line.split('\\t')\n",
    "    labels.append(int(label))\n",
    "    train_data.append(text.replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "xN3qjwcL4DoG"
   },
   "outputs": [],
   "source": [
    "counter = torch.load(ROOT+'counter.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "CjimMEy17mfR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zIqpwDtH4Lkj"
   },
   "outputs": [],
   "source": [
    "pre_count=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PhQWHNVD4UzV"
   },
   "outputs": [],
   "source": [
    "for line in train_data:\n",
    "  pre = []\n",
    "  for word in line:\n",
    "    pre.append(counter.get(word,0))\n",
    "  pre = pre +(MAX_LEN-len(pre))*[0]\n",
    "  pre_count.append(pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "y2zS61Xw7wsM"
   },
   "outputs": [],
   "source": [
    "pre_count = np.array(pre_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "UhmvboQj0e85"
   },
   "outputs": [],
   "source": [
    "for label in labels:\n",
    "  if label == 1:\n",
    "    weight.append(3)\n",
    "  else:\n",
    "    weight.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BF-N3Qx70t2_",
    "outputId": "5b695b03-1b5f-4886-e566-15ffba9105cb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "tokenizer=BertTokenizer.from_pretrained(BERT_TOK_ROOT)\n",
    "train_idx = []\n",
    "train_mask = []\n",
    "train_segment = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "c8F3czDY06DO"
   },
   "outputs": [],
   "source": [
    "for line in range(len(train_data)):\n",
    "  token = tokenizer.tokenize(train_data[line])\n",
    "  token = [\"[CLS]\"] + token + [\"[SEP]\"]\n",
    "  lenth.append(len(token))\n",
    "  if len(token)> MAX_LEN:\n",
    "      raise IndexError(\"Token length more than max seq length!\")\n",
    "\n",
    "  mask = [1]*len(token)+ [0]*(MAX_LEN-len(token))\n",
    "  sent = [0]*MAX_LEN\n",
    "  idx = tokenizer.convert_tokens_to_ids(token)\n",
    "  idx = idx + (MAX_LEN-len(idx))*[0]\n",
    "  train_idx.append(idx)\n",
    "  train_mask.append(mask)\n",
    "  train_segment.append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NJqHJxx7yc_L"
   },
   "source": [
    "# **统计特征**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "UYWjNA0Gyb5S"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "RgoHkWRkyPax"
   },
   "outputs": [],
   "source": [
    "text_encoder = Pipeline([\n",
    "    ('Text-TF-IDF', TfidfVectorizer(ngram_range=(1, 3))),\n",
    "    ('Text-SVD', TruncatedSVD(n_components = 600))], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "AqhWDvPW7ZIY"
   },
   "outputs": [],
   "source": [
    "text_encoder=torch.load(ROOT+'text_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "cLxbO6qH1kEV"
   },
   "outputs": [],
   "source": [
    "tfidf=text_encoder.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "r97WqYhH2oKu"
   },
   "outputs": [],
   "source": [
    "lenths = np.array(lenth)\n",
    "lenths=lenths.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0b9g5nBq2LjT"
   },
   "outputs": [],
   "source": [
    "use = lenths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "LJMdkUwn8i-2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NgpvlANC8il2"
   },
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "IPHste4V5mn4"
   },
   "outputs": [],
   "source": [
    "class EvalDataset(Dataset):\n",
    "  def __init__(self,train_idx,train_mask,train_segment,use,label=None):\n",
    "    self.train_idx = torch.tensor(train_idx,dtype=torch.int).long()\n",
    "    self.train_mask = torch.tensor(train_mask,dtype=torch.int).long()\n",
    "    self.train_segment = torch.tensor(train_segment,dtype=torch.int).long()\n",
    "    self.use = torch.tensor(use , dtype=torch.float32)\n",
    "    self.label = None\n",
    "    if label is not None:\n",
    "      self.label = torch.tensor(label,dtype=torch.float32)\n",
    "  def __len__(self):\n",
    "    return len(self.train_idx)\n",
    "  def __getitem__(self,item):\n",
    "    if self.label is not None:\n",
    "\n",
    "      output = {\n",
    "        'train_idx' : self.train_idx[item],\n",
    "        'train_mask' : self.train_mask[item],\n",
    "        'train_segment' :self.train_segment[item],\n",
    "        'use':self.use[item],\n",
    "        'label' : self.label[item]\n",
    "    }\n",
    "    else:\n",
    "      output = {\n",
    "        'train_idx' : self.train_idx[item],\n",
    "        'train_mask' : self.train_mask[item],\n",
    "        'use' : self.use[item],\n",
    "        'train_segment' :self.train_segment[item],\n",
    "      }\n",
    "    return {key: value for key, value in output.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "jP8nP-NxoyZ-"
   },
   "outputs": [],
   "source": [
    "def get_ids(x,y):\n",
    "  z=[]\n",
    "  for i in y:\n",
    "    z.append(x[i])\n",
    "  return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "T6xuWKfU7glP"
   },
   "outputs": [],
   "source": [
    "\n",
    "## Stolen from transformer code base without any noble intention.\n",
    "\n",
    "## Stolen from transformer code base without any noble intention.\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "        \n",
    "        weighted_input = x.permute(1,0,2) * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "class cnnbert(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config,kernel_sizes,num_channels,use_shape):\n",
    "        super(cnnbert, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.bn = nn.LayerNorm(768)\n",
    "        self.gelu=GELU()\n",
    "        self.att1=Attention(512,21)\n",
    "        self.att2=Attention(512,20)\n",
    "        self.att3=Attention(512,19)\n",
    "        self.att4=Attention(512,18)\n",
    "        self.att5=Attention(512,13)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        self.pre_cnn=nn.Linear(2*sum(num_channels)+use_shape,1)\n",
    "        self.linear = nn.Linear(2*sum(num_channels)+use_shape,2*sum(num_channels)+use_shape)\n",
    "\n",
    "        self.ln_cnn=nn.LayerNorm(sum(num_channels))\n",
    "        self.wt=torch.nn.Parameter(torch.rand((13,1),dtype=torch.float32,device=torch.device('cuda'),requires_grad=True))\n",
    "        nn.init.xavier_normal_(self.wt)\n",
    "        self.pool = GlobalAVGPool1d()\n",
    "        self.convs = nn.ModuleList()\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "          self.convs.append(nn.Conv1d(in_channels = 768,\n",
    "          out_channels = c,\n",
    "          kernel_size = k))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        use,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        \n",
    "    ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "           \n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        w1=outputs[2] \n",
    "        batch_size1,len_seq1,e_s1=w1[1].size()\n",
    "        bert_output1=w1[0].to(device)\n",
    "        bert_output1=bert_output1.unsqueeze(1)\n",
    "        for i in range(1,13):\n",
    "          bert_output1=torch.cat((bert_output1,w1[i].to(device).unsqueeze(1)),dim=1)\n",
    "        bert_output1=bert_output1.view(batch_size1,13,-1)\n",
    "        bert_output1=bert_output1.permute(0,2,1)\n",
    "        bert_output1=torch.matmul(bert_output1,((self.wt)))\n",
    "        bert_output1=bert_output1.view(batch_size1,len_seq1,e_s1)#下可接CNN或者att\n",
    "        bert_output1=self.bn(bert_output1)\n",
    "        bert_output1=bert_output1.permute(0, 2, 1)\n",
    "\n",
    "        bert_output1 = self.dropout(bert_output1)\n",
    "        conout1=self.att1(self.gelu(self.convs[0](bert_output1).permute(2,0,1)))\n",
    "        conout2=self.att2(self.gelu(self.convs[1](bert_output1).permute(2,0,1)))\n",
    "        conout3=self.att3(self.gelu(self.convs[2](bert_output1).permute(2,0,1)))\n",
    "        conout4=self.att4(self.gelu(self.convs[3](bert_output1).permute(2,0,1)))\n",
    "        conout5=self.att5(self.gelu(self.convs[4](bert_output1).permute(2,0,1)))\n",
    "        conout=torch.cat((conout1,conout2,conout3,conout4,conout5),dim=1)\n",
    "        encoding =torch.cat([self.pool(self.gelu(conv(bert_output1))).squeeze(-1) for conv in self.convs], dim=1)\n",
    "       \n",
    "        encoding=torch.cat((encoding,conout,use),dim=1)\n",
    "       \n",
    "        logits = self.pre_cnn((encoding))\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ahNNMKjJ8pcy"
   },
   "source": [
    "# **用于生成伪标签的模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HBE1jeyL8ouN"
   },
   "outputs": [],
   "source": [
    "'''class cnnbert(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config,kernel_sizes,num_channels,use_shape):\n",
    "        super(cnnbert, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.bn = nn.LayerNorm(768)\n",
    "        self.gelu=GELU()\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        self.pre_cnn=nn.Linear(1*sum(num_channels)+use_shape,1)\n",
    "        self.linear = nn.Linear(1*sum(num_channels)+use_shape,1*sum(num_channels)+use_shape)\n",
    "\n",
    "        self.ln_cnn=nn.LayerNorm(1*sum(num_channels))\n",
    "        self.wt=torch.nn.Parameter(torch.rand((13,1),dtype=torch.float32,device=torch.device('cuda'),requires_grad=True))\n",
    "        nn.init.xavier_normal_(self.wt)\n",
    "        self.pool = GlobalAVGPool1d()\n",
    "        self.convs = nn.ModuleList()\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "          self.convs.append(nn.Conv1d(in_channels = 768,\n",
    "          out_channels = c,\n",
    "          kernel_size = k))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        use,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        \n",
    "    ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "           \n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        w1=outputs[2] \n",
    "        batch_size1,len_seq1,e_s1=w1[1].size()\n",
    "        bert_output1=w1[0].to(device)\n",
    "        bert_output1=bert_output1.unsqueeze(1)\n",
    "        for i in range(1,13):\n",
    "          bert_output1=torch.cat((bert_output1,w1[i].to(device).unsqueeze(1)),dim=1)\n",
    "        bert_output1=bert_output1.view(batch_size1,13,-1)\n",
    "        bert_output1=bert_output1.permute(0,2,1)\n",
    "        bert_output1=torch.matmul(bert_output1,self.softmax((self.wt)))\n",
    "        bert_output1=bert_output1.view(batch_size1,len_seq1,e_s1)#下可接CNN或者att\n",
    "        bert_output1=self.bn(bert_output1)\n",
    "        bert_output1=bert_output1.permute(0, 2, 1)\n",
    "\n",
    "        bert_output1 = self.dropout(bert_output1)\n",
    "        encoding =torch.cat([self.pool(self.gelu(conv(bert_output1))).squeeze(-1) for conv in self.convs], dim=1)\n",
    "                            \n",
    "        encoding=torch.cat((encoding,use),dim=1)\n",
    "        \n",
    "        logits = self.pre_cnn((encoding))\n",
    "        return logits '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5r1L-rzm5lTY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PD1lhB935Yyk"
   },
   "outputs": [],
   "source": [
    "class GlobalAVGPool1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalAVGPool1d, self).__init__()\n",
    "    def forward(self, x):\n",
    "         # x shape: (batch_size, channel, seq_len)\n",
    "        return F.avg_pool1d(x, kernel_size=x.shape[2]) # shape: (batch_size, channel, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "OqwBAJlxBBY5"
   },
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper Section 3.4, last paragraph notice that BERT used the GELU instead of RELU\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 874
    },
    "colab_type": "code",
    "id": "yps1_9GYAnSj",
    "outputId": "dea82729-4b3b-4177-eb17-f1c3812664cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": null,\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"do_sample\": false,\n",
       "  \"eos_token_ids\": null,\n",
       "  \"finetuning_task\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"is_decoder\": false,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"length_penalty\": 1.0,\n",
       "  \"max_length\": 20,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_beams\": 1,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_labels\": 1,\n",
       "  \"num_return_sequences\": 1,\n",
       "  \"output_attentions\": false,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": null,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"pruned_heads\": {},\n",
       "  \"repetition_penalty\": 1.0,\n",
       "  \"temperature\": 1.0,\n",
       "  \"top_k\": 50,\n",
       "  \"top_p\": 1.0,\n",
       "  \"torchscript\": false,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_bfloat16\": false,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model_config = BERT_ROOT+'/bert_config.json'\n",
    "bert_config = BertConfig.from_json_file(bert_model_config)\n",
    "bert_config.num_labels = 1\n",
    "bert_config.output_hidden_states=True\n",
    "bert_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Z1rgNFnqLYur"
   },
   "outputs": [],
   "source": [
    "def my_loss(input,target,weight=None):\n",
    "  input=torch.sigmoid(input)\n",
    "  if weight is not None:\n",
    "    loss =torch._C._nn.binary_cross_entropy(input,target)\n",
    "    loss*=weight\n",
    "    return loss.sum()\n",
    "  else:\n",
    "    loss = torch._C._nn.binary_cross_entropy(input,target)\n",
    "    return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "N4cHJEMKC8hF"
   },
   "outputs": [],
   "source": [
    "def train_model(train_loader ,train_shape, optimizer ,scheduler, criterion, config=None):\n",
    "  model.train()\n",
    "  acc = 0\n",
    "  auc = 0\n",
    "  avg_loss = 0\n",
    "  optimizer.zero_grad()\n",
    "  for idx, batch in tqdm.tqdm(enumerate(train_loader),mininterval=2,desc='--Training',leave=False):\n",
    "    input_ids = batch['train_idx'].to(device)\n",
    "    input_mask = batch['train_mask'].to(device)\n",
    "    input_segments = batch['train_segment'].to(device)\n",
    "    label = batch['label'].to(device).view(-1)\n",
    "    use = batch['use'].to(device)\n",
    "    output_train = model(use=use,input_ids = input_ids.long(),\n",
    "                             labels = None,\n",
    "                             attention_mask = input_mask,\n",
    "                             token_type_ids = input_segments,)\n",
    "    loss = criterion(output_train.view(-1),label)                      \n",
    "    #loss = my_loss(output_train.view(-1),label,weight)\n",
    "    avg_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    acc += ((torch.sigmoid(output_train.cpu().view(-1))>0.5).int()==label.cpu()).sum()\n",
    "    #print(len(np.unique(label.cpu().int())))\n",
    "    #auc += sklearn.metrics.roc_auc_score( label.cpu(),torch.sigmoid(output_train.view(-1)).detach().cpu().numpy())\n",
    "\n",
    "    '''fgm.attack()\n",
    "    output_train = model(use,input_ids = input_ids.long(),\n",
    "                             labels = None,\n",
    "                             attention_mask = input_mask,\n",
    "                             token_type_ids = input_segments,)\n",
    "    loss_adv = criterion(output_train.view(-1),label)     \n",
    "    loss_adv.backward()\n",
    "    fgm.restore()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    '''\n",
    "    \n",
    "    ema.update()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "  return avg_loss/train_shape,acc.item()/train_shape,auc/len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "wV7kIFbfLGeH"
   },
   "outputs": [],
   "source": [
    "def valid_model(valid_loader ,val_shape,  criterion, batch_size ,config=None):\n",
    "  model.eval()\n",
    "  acc = 0\n",
    "  auc = 0\n",
    "  avg_loss = 0\n",
    "  pred = np.zeros((val_shape))\n",
    "  ema.apply_shadow()\n",
    "  with torch.no_grad():\n",
    "    for idx, batch in tqdm.tqdm(enumerate(valid_loader),mininterval=2,desc='--Valing',leave=False):\n",
    "      input_ids = batch['train_idx'].to(device)\n",
    "      input_mask = batch['train_mask'].to(device)\n",
    "      input_segments = batch['train_segment'].to(device)\n",
    "      label = batch['label'].to(device).view(-1)\n",
    "      use = batch['use'].to(device)\n",
    "      output_train = model(use=use,input_ids = input_ids.long(),\n",
    "                              labels = None,\n",
    "                              attention_mask = input_mask,\n",
    "                              token_type_ids = input_segments,\n",
    "                              )\n",
    "      loss = criterion(output_train.view(-1),label)\n",
    "      #loss = my_loss(output_train.view(-1),label,weight)\n",
    "      pred[idx*batch_size : (idx+1)*batch_size] = output_train.view(-1).detach().cpu().squeeze().numpy()\n",
    "      avg_loss += loss.item()\n",
    "      acc += ((torch.sigmoid(output_train.cpu().view(-1))>0.5).int()==label.cpu()).sum()\n",
    "      if input_ids.size()[0]!=1:\n",
    "\n",
    "        auc += sklearn.metrics.roc_auc_score( (label.cpu().int()),torch.sigmoid(output_train.view(-1)).detach().cpu().numpy())\n",
    "  ema.restore()\n",
    "  return avg_loss/val_shape,acc.item()/val_shape,auc/len(valid_loader),torch.sigmoid(torch.tensor(pred)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Z4XNaWdJMgV5"
   },
   "outputs": [],
   "source": [
    "class EMA():\n",
    "    def __init__(self, model, decay):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "\n",
    "    def register(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                self.backup[name] = param.data\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bSwGSYZW0E8h"
   },
   "outputs": [],
   "source": [
    "class FGM():\n",
    "  def __init__(self,model):\n",
    "    self.model=model\n",
    "    self.backup={}\n",
    "  def attack(self,epsilon=0.1,emb_name='word_emb'):\n",
    "    for name,param in self.model.named_parameters():\n",
    "      if param.requires_grad and emb_name in name:\n",
    "        self.backup[name]=param.data.clone()\n",
    "        if (param.grad)== None:\n",
    "          continue\n",
    "        norm=torch.norm(param.grad)\n",
    "        if norm!= 0 and not torch.isnan(norm):\n",
    "          r_at=epsilon * param.grad/norm\n",
    "          param.data.add_(r_at)\n",
    "  def restore(self,emb_name='word_emb'):\n",
    "    for name,param in self.model.named_parameters():\n",
    "      if param.requires_grad and emb_name in name:\n",
    "        assert name in self.backup\n",
    "        param.data=self.backup[name]\n",
    "    self.backup={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Lxles6wpzIdY"
   },
   "outputs": [],
   "source": [
    "class cycleOptimn():\n",
    "    def __init__(self,base_lr,max_lr,optimizer,stepsize):\n",
    "        self.optimizer=optimizer\n",
    "        self.base_lr=base_lr\n",
    "        self.max_lr=max_lr\n",
    "        self.stepsize=stepsize\n",
    "        self.current_step=0\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "    def step_and_update_lr(self):\n",
    "        self._update_learning_rate()\n",
    "        self.optimizer.step()\n",
    "    def _update_learning_rate(self):\n",
    "        self.current_step+=1\n",
    "\n",
    "        lr=self.get_triangular_lr(self.current_step,self.stepsize,self.base_lr,self.max_lr)\n",
    "        for param_group in  self.optimizer.param_groups:\n",
    "                param_group['lr']=lr\n",
    "       \n",
    "    def get_triangular_lr(self,iteration, stepsize, base_lr, max_lr):\n",
    "\n",
    "        cycle = np.floor(1 + iteration/(2  * stepsize))\n",
    "        x = np.abs(iteration/stepsize - 2 * cycle + 1)\n",
    "        lr = base_lr + (max_lr - base_lr) * np.maximum(0, (1-x))\n",
    "        return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ptmBNOKIvHat"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE =70\n",
    "NUM_EPOCH=5\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Zrkq8zsza-S-",
    "outputId": "114b451f-7c59-4ec0-e3ae-a7771773090f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla P100-PCIE-16GB'"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "KLs2-MgllhCH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4-7u7HOmlgRL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "aM6keeV3lfgQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "yG18WI_7kOr6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ag_nvU7MomRb"
   },
   "outputs": [],
   "source": [
    "floder = StratifiedKFold(n_splits=5,random_state=0,shuffle=False)\n",
    "predict = np.zeros(len(train_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "w2R6ABDhSR20"
   },
   "outputs": [],
   "source": [
    " \n",
    " \n",
    "def transfer_model(pretrained_file, model):\n",
    "    '''\n",
    "    只导入pretrained_file部分模型参数\n",
    "    tensor([-0.7119,  0.0688, -1.7247, -1.7182, -1.2161, -0.7323, -2.1065, -0.5433,-1.5893, -0.5562]\n",
    "    update:\n",
    "        D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
    "        If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
    "        If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
    "        In either case, this is followed by: for k in F:  D[k] = F[k]\n",
    "    :param pretrained_file:\n",
    "    :param model:\n",
    "    :return:\n",
    "    '''\n",
    "    pretrained_dict = torch.load(pretrained_file)  # get pretrained dict\n",
    "    model_dict = model.state_dict()  # get model dict\n",
    "    # 在合并前(update),需要去除pretrained_dict一些不需要的参数\n",
    "    pretrained_dict = transfer_state_dict(pretrained_dict, model_dict)\n",
    "    model_dict.update(pretrained_dict)  # 更新(合并)模型的参数\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    " \n",
    " \n",
    "def transfer_state_dict(pretrained_dict, model_dict):\n",
    "    '''\n",
    "    根据model_dict,去除pretrained_dict一些不需要的参数,以便迁移到新的网络\n",
    "    url: https://blog.csdn.net/qq_34914551/article/details/87871134\n",
    "    :param pretrained_dict:\n",
    "    :param model_dict:\n",
    "    :return:\n",
    "    '''\n",
    "    # state_dict2 = {k: v for k, v in save_model.items() if k in model_dict.keys()}\n",
    "    state_dict = {}\n",
    "    for k, v in pretrained_dict.items():\n",
    "        if k in model_dict.keys():\n",
    "            # state_dict.setdefault(k, v)\n",
    "            state_dict[k] = v\n",
    "        else:\n",
    "            print(\"Missing key(s) in state_dict :{}\".format(k))\n",
    "    return state_dict\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PnlchwOTms93"
   },
   "outputs": [],
   "source": [
    "wlabel_label=torch.load('/content/drive/My Drive/dl/推荐评论/CNN/w20_train_label_bin.pkl')\n",
    "wlabel_train_idx = torch.load('/content/drive/My Drive/dl/推荐评论/CNN/w20_train_idx.pkl')\n",
    "wlabel_train_mask = torch.load('/content/drive/My Drive/dl/推荐评论/CNN/w20_train_mask.pkl')\n",
    "wlabel_train_segment = torch.load('/content/drive/My Drive/dl/推荐评论/CNN/w20_train_segment.pkl')\n",
    "wlabel_train_use=torch.load('/content/drive/My Drive/dl/推荐评论/CNN/w20_train_lenth.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "d8rwwMvLn1f7"
   },
   "outputs": [],
   "source": [
    "use_shape = use.shape[1]\n",
    "use = list(use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "pc0thdpYZ6Li",
    "outputId": "00038568-3316-4719-9599-9f1e795882cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1/5 , train_loss: 0.0020144295440213175 ,train_acc:0.9409238776837996 ,train_auc: 0.0 \n",
      " \n",
      " val_loss :0.0029912285415949727 , val_acc :0.9075288972196188 , val_auc:0.9723019447566703 ,time: 501.6054196357727\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type cnnbert. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type GELU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type GlobalAVGPool1d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "--Training: 515it [04:20,  2.00it/s]"
     ]
    }
   ],
   "source": [
    "for fold,(train, test) in enumerate(floder.split(train_idx,labels)):\n",
    "  if fold == 3:\n",
    "    continue \n",
    "  print(fold)\n",
    "  t_idx = get_ids(train_idx , train)+wlabel_train_idx\n",
    "  t_mask = get_ids(train_mask , train)+wlabel_train_mask\n",
    "  t_segment = get_ids(train_segment , train)+wlabel_train_segment\n",
    "  t_weight = get_ids(weight , train)\n",
    "  #t_label = get_ids(labels , train)# (np.hstack((np.array(get_ids(labels , train)),wlabel_label)))\n",
    "  t_label = (np.vstack((np.array(get_ids(labels , train)).reshape(-1,1),wlabel_label.reshape(-1,1))))\n",
    "\n",
    "  t_use = (np.vstack((np.array(get_ids(use , train)),(wlabel_train_use).reshape(-1,1))))\n",
    "  #t_use=(np.array(get_ids(use , train)))\n",
    "  valid_idx = get_ids(train_idx , test)\n",
    "  valid_mask = get_ids(train_mask , test)\n",
    "  valid_segment = get_ids(train_segment , test)\n",
    "  valid_weight = get_ids(weight , test)\n",
    "  valid_label = get_ids(labels , test)\n",
    "  valid_use = get_ids(use , test)\n",
    "  train_shape = len(t_idx)\n",
    "  valid_shape = len(valid_idx)\n",
    "  train_set = EvalDataset(t_idx , t_mask , t_segment , t_use ,t_label)\n",
    "  vla_set = EvalDataset(valid_idx , valid_mask , valid_segment ,valid_use ,valid_label)\n",
    "  train_loader = DataLoader (train_set , batch_size=BATCH_SIZE,shuffle=True)\n",
    "  valid_loader = DataLoader (vla_set , batch_size=BATCH_SIZE,shuffle=False)\n",
    "  kernel_sizes,nums_channels = [2,3,4,5,10], [512,512,512,512,512]\n",
    "  model = cnnbert.from_pretrained(BERT_ROOT,kernel_sizes=kernel_sizes,num_channels=nums_channels,use_shape=use_shape,config = bert_config)\n",
    "  #model = transfer_model('/content/drive/My Drive/dl/美团/CLS/best_param_score_1',model)\n",
    "  model.zero_grad();\n",
    "  model.to(device);\n",
    "  #fgm = FGM(model)\n",
    "  ema=EMA(model,0.995)\n",
    "  ema.register()\n",
    "  def is_backbone(n):\n",
    "        return \"bert\" in n\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, eps=4e-5,weight_decay=0.8)\n",
    "  scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0.05, num_training_steps= 5*len(train_loader))\n",
    "  #optimizer=cycleOptimn(0.00001,0.0001,torch.optim.AdamW(model.parameters(), lr=0.00002, eps=4e-5,weight_decay=0.8),90)\n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "  best_avg_loss = -100\n",
    "  best_score = -1\n",
    "  best_param_score = None\n",
    "  for epoch in range(NUM_EPOCH):\n",
    "    torch.cuda.empty_cache()\n",
    "    start_time = time.time()\n",
    "    train_loss,train_acc,train_auc=train_model(train_loader ,train_shape, optimizer , scheduler, criterion, config=None)\n",
    "    val_loss,val_acc,val_auc,pred=valid_model(valid_loader ,valid_shape,  criterion, BATCH_SIZE ,config=None)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('epoch{}/{} , train_loss: {} ,train_acc:{} ,train_auc: {} \\n \\n val_loss :{} , val_acc :{} , val_auc:{} ,time: {}\\n'.format(epoch+1,NUM_EPOCH,train_loss,train_acc,train_auc,val_loss,val_acc,val_auc,elapsed_time))\n",
    "    if best_score < val_auc:\n",
    "      best_score = val_auc\n",
    "      best_param_score = model.state_dict()\n",
    "      predict[test] = pred\n",
    "      torch.save(best_param_score,'/content/drive/My Drive/dl/推荐评论/0.96840版本基础 伪标签/CNN_attack0.1/best_param_score{}.pkl'.format(fold+1))\n",
    "      torch.save(ema,'/content/drive/My Drive/dl/推荐评论/0.96840版本基础 伪标签/CNN_attack0.1/EMA{}.pkl'.format(fold+1))\n",
    "\n",
    "  del model, optimizer, criterion, scheduler\n",
    "  torch.cuda.empty_cache()\n",
    "  del valid_loader, train_loader, vla_set, train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "h9KT3u2Dj2pT"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "plru6T3t0-wO"
   },
   "outputs": [],
   "source": [
    "sklearn.metrics.roc_auc_score(labels,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "8I6SzLuj5XT0"
   },
   "outputs": [],
   "source": [
    "0.9712348313293575"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ymPqBQyCNVmS"
   },
   "source": [
    "+att 0.96822\n",
    "\n",
    "无att: 0.9684   lr:0.00005\n",
    "\n",
    "+预训练: 不行\n",
    "\n",
    "+use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "97bMTXYnNR5Y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
